<pre class='metadata'>
Title: Private Aggregation API
Shortname: private-aggregation-api
Level: 1
Status: w3c/CG-DRAFT
Group: patcg
Repository: patcg-individual-drafts/private-aggregation-api
URL: https://patcg-individual-drafts.github.io/private-aggregation-api
Editor: Alexander Turner, Google https://www.google.com, alexmt@chromium.org
Abstract: A generic API for measuring aggregate, cross-site data in a privacy
    preserving manner. The potentially identifying cross-site data is
    encapsulated into <em>aggregatable reports</em>. To prevent leakage, this
    data is encrypted, ensuring it can only be processed by an <em>aggregation
    service</em>. During processing, this service will add noise and impose
    limits on how many queries can be performed.

<!--- Warning: Not Ready -->
Markup Shorthands: markdown on
Complain About: accidental-2119 on, missing-example-ids on
Assume Explicit For: on
</pre>

<pre class="anchors">
urlPrefix: https://wicg.github.io/turtledove/; type: interface
    text: InterestGroupBiddingScriptRunnerGlobalScope
    text: InterestGroupScriptRunnerGlobalScope
    text: InterestGroupScoringScriptRunnerGlobalScope
    text: InterestGroupReportingScriptRunnerGlobalScope
urlPrefix: https://wicg.github.io/shared-storage/; type: interface
    text: SharedStorageWorklet
    text: SharedStorageWorkletGlobalScope
spec: hr-time; type: dfn; urlPrefix: https://w3c.github.io/hr-time/
    text: current wall time; url: #dfn-current-wall-time
    text: duration; url: #dfn-duration
    text: duration from; url: #dfn-duration-from
    text: moment; url: #dfn-moment
    text: unix epoch; url: #dfn-unix-epoch
    text: wall clock; url: #dfn-wall-clock
spec: html; type: dfn; urlPrefix: https://tc39.es/ecma262/
    text: call; url: sec-call
</pre>

<pre class=link-defaults>
spec:infra; type:dfn; text:user agent
</pre>


Introduction {#intro}
=====================

<em>This section is non-normative.</em>

Motivation {#motivation}
------------------------

Browsers are now working to prevent cross-site user tracking, including by
partitioning storage and removing third-party cookies. There are a range of API
proposals to continue supporting legitimate use cases in a way that respects
user privacy. Many of these APIs, including the
<a href="https://wicg.github.io/shared-storage/">Shared Storage API</a> and the
<a href="https://wicg.github.io/turtledove/">Protected Audience API</a>, isolate
potentially identifying cross-site data in special contexts, which ensures that
the data cannot escape the user agent.

Relative to cross-site data from an individual user, aggregate data about groups
of users can be less sensitive and yet would be sufficient for a wide range of
use cases. An aggregation service has been proposed to allow reporting noisy,
aggregated cross-site data. This service was originally proposed for use by the
<a href="https://wicg.github.io/attribution-reporting-api/">Attribution
Reporting API</a>, but allowing more general aggregation would support
additional use cases. In particular, the Protected Audience and Shared Storage
proposals expect this functionality to become available.

Overview {#overview}
--------------------

This document oulines a general-purpose API that can be called from isolated
contexts that have access to cross-site data (such as a Shared Storage worklet).
Within these contexts, potentially identifying data can be encapsulated into
"aggregatable reports". To prevent leakage, the cross-site data in these reports
is encrypted to ensure it can only be processed by the aggregation service.
During processing, this service adds noise and imposes limits on how many
queries can be performed.

This API provides functions allowing the origin to construct an aggregatable
report and specify the values to be embedded into its encrypted payload (for
later computation via the aggregation service). These calls result in the
aggregatable report being queued to be sent to the reporting endpoint of the
script's origin after a delay. After the endpoint receives the reports, it will
batch the reports and send them to the aggregation service for processing. The
output of that process is a summary report containing the (approximate) result,
which is dispatched back to the script's origin.

Considered alternative {#considered-alternative}
------------------------------------------------

Instead of the above API shape, we considered instead aligning with a design
that is much closer to {{WindowOrWorkerGlobalScope/fetch()}}. However, there are
a few key differences which made this unfavorable:
- This API is designed to be used in isolated contexts where
    {{WindowOrWorkerGlobalScope/fetch()}} is not available.
- It's an anti-goal to give the developer control over when [=aggregatable
    reports=] are being sent or knowledge that they were sent (outside of the
    isolated context). (Except when providing a context ID from <em>outside</em>
    the isolated context, see [Protecting against leaks via the number of
    reports](#protecting-against-leaks-via-the-number-of-reports) below.
- The reports cannot be sent to arbitrary reporting endpoints, only a
    particular `.well-known` path on the script origin.
- The report's input is very specific (an array of {{PAHistogramContribution}}s)
    and is not amenable to {{WindowOrWorkerGlobalScope/fetch()}}'s general
    purpose contents.
- There is no concept of a response.

So, we chose the more tailored API shape detailed below.

General methods {#general-methods}
==================================

Worklet interface {#worklet-interface}
--------------------------------------

<xmp class="idl">
[Exposed=(InterestGroupScriptRunnerGlobalScope,SharedStorageWorklet),
 SecureContext]
interface PrivateAggregation {
  undefined contributeToHistogram(PAHistogramContribution contribution);
  undefined enableDebugMode(optional PADebugModeOptions options = {});
};

dictionary PAHistogramContribution {
  required bigint bucket;
  required long value;
};

dictionary PADebugModeOptions {
  required bigint debugKey;
};
</xmp>

Issue: Need to spec Permissions Policy integration.

Each {{PrivateAggregation}} object has a field named <dfn
for="PrivateAggregation">scoping details</dfn> that is a [=scoping details=] or
null (default null).

Note: See [Exposing to global scopes](#exposing) below.

<div algorithm>
The <dfn method for="PrivateAggregation">
contributeToHistogram(PAHistogramContribution contribution)</dfn> method steps
are:
</div>

1. Throw {{RangeError}} if |contribution|["{{PAHistogramContribution/bucket}}"]
    is not in the range [0, 2<sup>128</sup>−1].
1. Throw {{RangeError}} if |contribution|["{{PAHistogramContribution/value}}"]
    is negative.
1. Let |scopingDetails| be the {{PrivateAggregation}} object's
    [=PrivateAggregation/scoping details=].
1. If |scopingDetails| is null, throw a new {{DOMException}} with name
    "`NotAllowedError`".

    Note: This indicates the API is not yet available, for example, because the
        initial execution of the script after loading is not complete.

    Issue: Consider improving developer ergonomics here (e.g. a way to detect
        this case).

1. Let |entry| be a new [=contribution cache entry=] with the items:
    : [=contribution cache entry/contribution=]
    :: |contribution|
    : [=contribution cache entry/batching scope=]
    :: The result of running |scopingDetails|' [=scoping details/get batching
        scope steps=].
    : [=contribution cache entry/debug scope=]
    :: The result of running |scopingDetails|' [=scoping details/get debug scope
        steps=].

1. [=Append an entry to the contribution cache|Append=] |entry| to the
    [=contribution cache=].

Issue: Check that value can actually be zero in the spec pipeline.

Issue(44): Consider accepting an array of contributions.

<div algorithm>
The <dfn method for="PrivateAggregation">
enableDebugMode(optional PADebugModeOptions options)</dfn> method steps are:
</div>

1. Let |scopingDetails| be the {{PrivateAggregation}} object's
    [=PrivateAggregation/scoping details=].
1. If |scopingDetails| is null, throw a new {{DOMException}} with name
    "`NotAllowedError`".
1. Let |debugScope| be the result of running |scopingDetails|' [=scoping
    details/get debug scope steps=].
1. Let |debugScopeMap| be the [=debug scope map=].
1. Throw {{DataError}} if |debugScopeMap|[|debugScope|] [=map/exists=].

    Note: This would occur if `enableDebugMode()` has already been run for this
        [=debug scope=].
1. Let |debugKey| be null.
1. If |options| was given:
    1. Throw {{DataError}} if |options|["{{PADebugModeOptions/debugKey}}] is not
        in the range [0, 2<sup>64</sup>−1].
    1. Set |debugKey| to |options|["{{PADebugModeOptions/debugKey}}].
1. Let |debugDetails| be a new [=debug details=] with the items:
    : [=debug details/enabled=]
    :: true
    : [=debug details/key=]
    :: |debugKey|

1. [=map/Set=] |debugScopeMap|[|debugScope|] to |debugDetails|.

Exposing to global scopes {#exposing}
=====================================

To expose this API to a global scope, a readonly attribute `privateAggregation`
of type {{PrivateAggregation}} should be exposed on the global scope.

Additionally, each global scope should set the [=PrivateAggregation/scoping
details=] for the {{PrivateAggregation}} object it exposes to a non-null value.
The global scope should wait to set the field until the API is intended to be
available.

<div class="example" id="shared-storage-only-within-operations">
    Shared Storage only allows Private Aggregation when an operation is being
    invoked, not in the top-level context:
    <pre highlight="js">
    class ExampleOperation {
      async run(data) {
        privateAggregation.contributeToHistogram(...)  // This is allowed.
      }
    }
    register('example-operation', ExampleOperation);

    privateAggregation.contributeToHistogram(...)  // This would cause an error.
    </pre>
    So, Shared Storage sets the [=PrivateAggregation/scoping details=] after the
    initial execution of the module script is complete.
</div>

For any [=batching scope=] returned by the [=scoping details/get batching scope
steps=], the [=process contributions for a batching scope=] steps should later
be performed given that same batching scope, the global scope's [=relevant
settings object=]'s [=environment settings object/origin=] and some [=context
type=].

Note: This last requirement means that global scopes with different origins
    cannot share the same batching scope, see Same-origin policy discussion.

    Issue: Link Same-origin policy section.

For any [=debug scope=] returned by the [=scoping details/get debug scope
steps=], the [=mark a debug scope complete=] steps should later be performed
given that same [=debug scope=].

Note: A later algorithm [=asserts=] that, for any [=contribution cache entry=]
    in the [=contribution cache=], the [=mark a debug scope complete=] steps
    were performed given the entry's [=contribution cache entry/debug scope=]
    before the [=process contributions for a batching scope=] steps are
    performed given the entry's [=contribution cache entry/batching scope=].

Structures {#structures}
========================

General {#general-structures}
-----------------------------

<h4 dfn-type=dfn>Batching scope</h4>
A batching scope is a <a spec=HTML>unique internal value</a> that identifies
which {{PAHistogramContribution}}s should be sent in the same [=aggregatable
report=] unless their [=aggregatable report/debug details=] differ.

Issue: Unique internal value is not an exported definition. See
    <a href="https://github.com/whatwg/infra/issues/583">infra/583</a>.

<h4 dfn-type=dfn>Debug scope</h4>
A debug scope is a <a spec=HTML>unique internal value</a> that identifies which
{{PAHistogramContribution}}s should share the same [=debug details=].

<h4 dfn-type=dfn>Scoping details</h4>
A scoping details is a [=struct=] with the following items:
<dl dfn-for="scoping details">
: <dfn>get batching scope steps</dfn>
:: An algorithm returning a [=batching scope=]
: <dfn>get debug scope steps</dfn>
:: An algorithm returning a [=debug scope=]

</dl>

<h4 dfn-type=dfn>Debug details</h4>
A debug details is a [=struct=] with the following items:
<dl dfn-for="debug details">
: <dfn>enabled</dfn> (default false)
:: A [=boolean=]
: <dfn>key</dfn> (default null)
:: Null or an unsigned 64-bit integer. The key must be null if enabled is false.

</dl>

<h4 dfn-type=dfn>Contribution cache entry</h4>
A contribution cache entry is a [=struct=] with the following items:
<dl dfn-for="contribution cache entry">
: <dfn>contribution</dfn>
:: A {{PAHistogramContribution}}
: <dfn>batching scope</dfn>
:: A [=batching scope=]
: <dfn>debug scope</dfn>
:: A [=debug scope=]
: <dfn>debug details</dfn>
:: A [=debug details=] or null (default null)

</dl>

<h4 dfn-type=dfn>Aggregatable report</h4>

An aggregatable report is a [=struct=] with the following items:
<dl dfn-for="aggregatable report">
: <dfn>reporting origin</dfn>
:: An [=origin=]
: <dfn>original report time</dfn>
:: A [=moment=]
: <dfn>report time</dfn>
:: A [=moment=]
: <dfn>contributions</dfn>
:: A [=list=] of {{PAHistogramContribution}}s
: <dfn>api</dfn>
:: A [=context type=]
: <dfn>report ID</dfn>
:: A [=string=]
: <dfn>debug details</dfn>
:: A [=debug details=]
: <dfn>queued</dfn>
:: A [=boolean=]

</dl>

Issue: Handle context ID, maybe retries/offline

<h4 dfn-type=dfn>Context type</h4>
A context type is a [=string=] indicating what kind of global scope the
{{PrivateAggregation}} object was exposed in.

Storage {#storage}
==================

A user agent holds a <dfn>contribution cache</dfn>, which is a [=list=] of
[=contribution cache entries=].

A user agent holds an <dfn>aggregatable report cache</dfn>, which is a [=list=]
of [=aggregatable reports=].

A user agent holds a <dfn>debug scope map</dfn>, which is an [=ordered map=]
from [=debug scopes=] to [=debug details=].

Clearing storage {#clearing-storage}
----------------------------------------

The user agent must expose controls that allow the user to delete data from
the [=aggregatable report cache=] as well as any contribution history data
stored for the [=consume budget if permitted=] algorithm.

The user agent may expose controls that allow the user to delete data from the
[=contributions cache=] and the [=debug scope map=].

[=Implementation-defined=] values {#implementation-defined-values}
==================================================================

<dfn>Maximum report contributions</dfn> is a positive integer that controls how
many contributions can be present in a single report.

<dfn>Minimum report delay</dfn> is a non-negative [=duration=] that controls the
minimum delay to deliver an [=aggregatable report=]

<dfn>Randomized report delay</dfn> is a positive [=duration=] that controls the
random delay to deliver an [=aggregatable report=]. This delay is additional to
the minimum delay.

Issue: More

Algorithms {#algorithms}
====================

To <dfn>serialize an integer</dfn>, represent it as a [=string=] of the shortest
possible decimal number.

Issue: This would ideally be replaced by a more descriptive algorithm in Infra.
    See <a href="https://github.com/whatwg/infra/issues/201">infra/201</a>.

Exported algorithms {#exported-algorithms}
------------------------------------------

Note: These algorithms allow other specifications to integrate with this API.

To <dfn algorithm export>append an entry to the contribution cache</dfn> given a
[=contribution cache entry=] |entry|:
1. [=list/Append=] |entry| to the [=contribution cache=].

To <dfn algorithm export>mark a debug scope complete</dfn> given a [=debug
scope=] |debugScope|:

1. Let |debugDetails| be a new [=debug details=].
1. If [=debug scope map=][|debugScope|] [=map/exists=]:
    1. Set |debugDetails| to  [=debug scope map=][|debugScope|].
    1. [=map/Remove=] [=debug scope map=][|debugScope|].
    1. If |debugDetails|'s [=debug details/key=] is not null, [=assert=]
        |debugDetails|'s [=debug details/enabled=] is true.
1. [=list/iterate|For each=] |entry| of the [=contribution cache=]:
    1. If |entry|'s [=contribution cache entry/debug scope=] is |debugScope|,
        set |entry|'s [=contribution cache entry/debug details=] to
        |debugDetails|.

To <dfn algorithm export>process contributions for a batching scope</dfn> given
a [=batching scope=] |batchingScope|, an [=origin=] |reportingOrigin| and a
[=context type=] |contextType|:
1. Let |batchEntries| be a new [=list/is empty|empty=] [=list=].
1. [=list/iterate|For each=] |entry| of the [=contribution cache=]:
    1. If |entry|'s [=contribution cache entry/batching scope=] is
        |batchingScope|:
        1. [=Assert=]: |entry|'s [=contribution cache entry/debug details=] is
            not null.

            Note: This asserts that the [=mark a debug scope complete=] steps
                were run before the [=process contributions for a batching
                scope=] steps.

        1. [=list/Append=] |entry| to |batchEntries|.
1. If |batchEntries| is [=list/empty=], return.
1. Let |batchedContributions| be a new [=map/is empty|empty=] [=ordered map=].
1. [=list/iterate|For each=] |entry| of |batchEntries|:
    1. [=list/Remove=] |entry| from the [=contribution cache=].
    1. Let |debugDetails| be |entry|'s [=contribution cache entry/debug
        details=].
    1. If |batchedContributions|[|debugDetails|] does not [=map/exist=]:
        1. [=map/Set=] |batchedContributions|[|debugDetails|] to a new [=list/is
            empty|empty=] [=list=].
    1. [=list/Append=] |entry|'s [=contribution cache entry/contribution=] to
        |batchedContributions|[|debugDetails|].
1. [=map/iterate|For each=] |debugDetails| → |contributions| of
    |batchedContributions|:
    1. Perform the [=report creation and scheduling steps=] with
        |reportingOrigin|, |contextType|, |contributions| and |debugDetails|.

Note: These steps break up the contributions based on their [=debug details=] as
    each report can only have one set of metadata.

Scheduling reports {#scheduling-reports}
----------------------------------------

To perform the <dfn algorithm>report creation and scheduling steps</dfn> with an
[=origin=] |reportingOrigin|, a [=context type=] |api|, a [=list=] of
{{PAHistogramContribution}}s |contributions| and a [=debug details=]
|debugDetails|:
1. [=Assert=]: |reportingOrigin| is a [=potentially trustworthy origin=].
1. Let |truncatedContributions| be a new [=list/is empty|empty=] [=list=].
1. If |contributions| has a [=list/size=] greater than [=maximum report
    contributions=]:
    1. [=set/For each=] |n| of [=the exclusive range|the range=] 0 to [=maximum
        report contributions=], exclusive:
        1. [=set/Append=] |contributions|[|n|] to |truncatedContributions|.
1. Otherwise, set |truncatedContributions| to |contributions|.
1. Let |contributionSum| be 0.
1. [=set/iterate|For each=] |contribution| of |truncatedContributions|:
    1. [=Assert=]: |contribution|["|value|"] is non-negative.
    1. Add |contribution|["|value|"] to |contributionSum|.
1. Let |currentWallTime| be the [=current wall time=].
1. Let |sufficientBudget| be the result of [=consuming budget if permitted=]
    given |contributionSum|, |reportingOrigin|, |api| and |currentWallTime|.
1. If |sufficientBudget| is false, return.
1. Let |report| be the result of [=obtaining an aggregatable report=] given
    |reportingOrigin|, |api|, |truncatedContributions|, |debugDetails| and
    |currentWallTime|.
1. [=set/Append=] |report| to the user agent's [=aggregatable report cache=].

Issue: Do we need to ensure the reports aren't queued after being sent?

Issue: Do we need to address user settings here at all?

To <dfn algorithm>consume budget if permitted</dfn> given a {{long}} |value|, an
[=origin=] <var ignore=''>origin</var>, a [=context type=] |api| and a
[=moment=] |currentTime|, perform [=implementation-defined=] steps. They return
a [=boolean=], which indicates whether there is sufficient 'contribution budget'
left to send the requested contribution |value|. This budget should be bound to
usage over time, e.g. the contribution sum over the last 24 hours. The algorithm
should assume that the contribution will be sent if and only if true is
returned, i.e. it should consume the budget in that case.

To <dfn>obtain an aggregatable report</dfn> given an [=origin=]
|reportingOrigin|, a [=context type=] |api|, a [=list=] of
{{PAHistogramContribution}}s |contributions|, a [=debug details=]
|debugDetails|, and a [=moment=] |currentTime|, perform the following steps.
They return an [=aggregatable report=].
1. [=Assert=]: |reportingOrigin| is a [=potentially trustworthy origin=].
1. Let |reportTime| be the result of running [=obtain a report delivery time=]
    given |currentTime|.
1. Let |report| be a new [=aggregatable report=] with the items:
    : [=aggregatable report/reporting origin=]
    :: |reportingOrigin|
    : [=aggregatable report/original report time=]
    :: |reportTime|
    : [=aggregatable report/report time=]
    :: |reportTime|
    : [=aggregatable report/contributions=]
    :: |contributions|
    : [=aggregatable report/api=]
    :: |api|
    : [=aggregatable report/report ID=]
    :: The result of [=generating a random UUID=].
    : [=aggregatable report/debug details=]
    :: |debugDetails|
    : [=aggregatable report/queued=]
    :: false
1. Return |report|.

To <dfn algorithm>obtain a report delivery time</dfn> given a [=moment=]
|currentTime|, perform the following steps. They return a [=moment=].
1. Let |r| be a random double between 0 (inclusive) and 1 (exclusive) with
    uniform probability.
1. Return |currentTime| + [=minimum report delay=] + |r| * [=randomized report
    delay=].

Sending reports {#sending-reports}
----------------------------------

Note: This section is largely copied from the
    <a href="https://wicg.github.io/attribution-reporting-api/">Attribution
    Reporting API spec</a>, adapting as necessary.

Issue: Add logic for resetting [=aggregatable report/queued=] on start up or
    network on or whatever

Issue: Do we have to use the [=queue a task=] algorithm here?

The user agent must periodically [=attempt to queue reports for sending=] with
its [=aggregatable report cache=].

To <dfn>attempt to queue reports for sending</dfn> given a [=list=] of
[=aggregatable reports=] |reports|:
1. [=list/iterate|For each=] |report| of |reports|, run these steps [=in
    parallel=]:
    1. Run these steps, but [=abort when=] the [=user agent=] shuts down:
        1. If |report|'s [=aggregatable report/queued=] value is true, return.
        1. Set |report|'s [=aggregatable report/queued=] value to true.
        1. Let |currentWallTime| be the [=current wall time=].
        1. If |report|'s [=aggregatable report/report time=] is before
            |currentWallTime|, set |report|'s [=aggregatable report/report
            time=] to |currentWallTime| plus an [=implementation-defined=]
            random non-negative [=duration=].

            Note: On startup, it is possible the user agent will need to send
                many reports whose report times passed while the browser was
                closed. Adding random delay prevents temporal joining of
                reports.
        1. Wait until the [=current wall time=] is equal to or after |report|'s
            [=aggregatable report/report time=].
        1. Optionally, wait a further [=implementation-defined=] non-negative
            [=duration=].

            Note: This is intended to allow user agents to optimize device
                resource usage.
        1. Run [=attempt to deliver a report=] with |report|.
    1. [=If aborted=], set |report|'s [=aggregatable report/queued=] value to
        false.

        Note: It might be more practical to perform these steps when the [=user
            agent=] next starts up.

To <dfn>attempt to deliver a report</dfn> given an [=aggregatable report=]
|report|:
1. Let |url| be the result of [=obtaining a reporting endpoint=] given
    |report|'s [=aggregatable report/reporting origin=] and |report|'s
    [=aggregatable report/api=].
1. Let |data| be the result of [=serializing an aggregatable report=] given
    |report|.
1. If |data| is an error, [=list/remove=] |report| from the [=aggregatable
    report cache=]. (TODO: do we need to queue this task?)
1. Let |request| be the result of [=creating a report request=] given |url| and
    |data|.
1. [=Queue a task=] to [=fetch=] |request| with [=fetch/processResponse=] being
    the following steps:
    1. Let |shouldRetry| be an [=implementation-defined=] [=boolean=]. The value
        should be false if no error occurred.
    1. If |shouldRetry| is true, set |report|'s [=aggregatable report/queued=]
        value to false.
    1. Otherwise, [=list/remove=] |report| from the [=aggregatable report
        cache=].

To <dfn>obtain a reporting endpoint</dfn> given an [=origin=] |reportingOrigin|
and [=context type=] |api|, perform the following steps. They return a [=URL=].
1. [=Assert=]: |reportingOrigin| is a [=potentially trustworthy origin=].
1. Let |path| be the [=string/concatenation=] of
    «"<code>.well-known/private-aggregation/report-</code>", |api|».
1. Let |base| be the result on running the [=URL parser=] on the [=serialization
    of an origin|serialization=] of |reportingOrigin|.
1. [=Assert=]: |base| is not failure.
1. Let |result| be the result of running the [=URL parser=] on |path| with
    |base|.
1. [=Assert=]: |result| is not failure.
1. Return |result|.

To <dfn>create a report request</dfn> given a [=URL=] |url| and a [=byte
sequence=] |body|:
1. Let |request| be a new [=request=] with the following properties:
    :   [=request/method=]
    ::  "`POST`"
    :   [=request/URL=]
    ::  |url|
    :   [=request/header list=]
    ::  «("`Content-Type`", "`application/json`")»
    :   [=request/unsafe-request flag=]
    ::  set
    :   [=request/body=]
    ::  |body|
    :   [=request/client=]
    ::  `null`
    :   [=request/window=]
    ::  "`no-window`"
    :   [=request/service-workers mode=]
    ::  "`none`"
    :   [=request/initiator=]
    ::  ""
    :   [=request/referrer=]
    :: "`no-referrer`"
    :   [=request/mode=]
    ::  "`cors`"
    :   [=request/credentials mode=]
    ::  "`omit`"
    :   [=request/cache mode=]
    ::  "`no-store`"
1. Return |request|.

Serializing reports {#serializing-reports}
------------------------------------------

Note: This section is largely copied from the
    <a href="https://wicg.github.io/attribution-reporting-api/">Attribution
    Reporting API spec</a>, adapting as necessary.

To <dfn>serialize an aggregatable report</dfn> given an [=aggregatable report=]
|report|, perform the following steps. They return a [=byte sequence=] or an
error.
1. Let |aggregationServicePayloads| be the result of [=obtaining the aggregation
    service payloads=] given |report|.
1. If |aggregationServicePayloads| is an error, return
    |aggregationServicePayloads|.
1. Let |data| be an [=ordered map=] of the following key/value pairs:
    : "`aggregation_service_payloads`"
    :: |aggregationServicePayloads|
    : "`shared_info`"
    :: The result of [=obtaining a report's shared info=] given |report|.
1. Let |debugKey| be |report|'s [=aggregatable report/debug details=]'s [=debug
    details/key=].
1. If |debugKey| is not null:
    1. [=map/Set=] |data|["`debug_key`"] to |debugKey|.
1. Return the [=byte sequence=] resulting from executing [=serialize an infra
    value to JSON bytes=] on |data|.

To <dfn>obtain the aggregation service payloads</dfn> given an [=aggregatable
report=] |report|, perform the following steps. They return a [=list=] of
[=maps=] or an error.
1. Let (|pkR|, |keyId|) be the result of [=obtaining the public key for
    encryption=].
1. If |pkR| is an error, return |pkR|.
1. Let |plaintextPayload| be the result of [=obtaining the plaintext payload=]
    given |report|.
1. Let |encryptedPayload| be the result of [=obtaining the encrypted payload=]
    given |plaintextPayload| and |pkR|.
1. If |encryptedPayload| is an error, return |encryptedPayload|.
1. Let |aggregationServicePayloads| be a new [=list/is empty|empty=] [=list=].
1. Let |aggregationServicePayload| be an [=ordered map=] of the following
    key/value pairs:
    : "`key_id`"
    :: |keyId|
    : "`payload`"
    :: |encryptedPayload|, [=forgiving-base64 encode|base64 encoded=]
1. If |report|'s [=aggregatable report/debug details=]'s [=debug details/
    enabled=] field is true:
    1. [=map/Set=] |aggregationServicePayload|[`debug_cleartext_payload`] to
        |plaintextPayload|, [=forgiving-base64 encode|base64 encoded=].
1. [=list/Append=] |aggregationServicePayload| to |aggregationServicePayloads|.
1. Return |aggregationServicePayloads|.

To <dfn>obtain the public key for encryption</dfn>, asynchronously perform an
[=implementation-defined=] sequence of steps. They return a [=tuple=] consisting
of a public key and a [=string=] (which should uniquely identify the public
key), or an error in the event that the [=user agent=] failed to obtain the
public key. (TODO: why asynchronously.)

Note: The user agent is encouraged to enforce regular key rotation. If there are
    multiple keys, the user agent can independently pick a key uniformly at
    random for every encryption operation.

To <dfn>obtain the plaintext payload</dfn> given an [=aggregatable report=]
    |report|, perform the following steps. They return a [=byte sequence=].
1. Let |payloadData| be a new [=list/is empty|empty=] [=list=].
1. [=list/iterate|For each=] |contribution| of |report|'s [=aggregatable report/
    contributions=]:
    1. Let |contributionData| be an [=ordered map=] of the following key/value
        pairs:
        : "`bucket`"
        :: The result of [=encoding an integer for the payload=] given
            |contribution|["{{PAHistogramContribution/bucket}}"] and 128.
        : "`value`"
        :: The result of [=encoding an integer for the payload=] given
            |contribution|["{{PAHistogramContribution/value}}"] and 32.
    1. [=list/Append=] |contributionData| to |payloadData|.
1. Let |payload| be an [=ordered map=] of the following key/value pairs:
    : "`data`"
    :: |payloadData|
    : "`operation`"
    :: "`histogram`"
1. Return the [=byte sequence=] resulting from [[!RFC8949|CBOR encoding]]
    |payload|.

To <dfn>obtain the encrypted payload</dfn> given a [=byte sequence=]
|plaintextPayload|, perform the following steps. They return a [=byte sequence=]
or an error.

1. Let |sharedInfo| be the result of [=obtaining a report's shared info=] given
    |report|.
1. Let |info| be the result of [=UTF-8 encoding=] the [=string/concatenation=]
    of «"`aggregation_service`", |sharedInfo|».
1. Let (|kem_id|, |kdf_id|, |aead_id|) be (0x0020, 0x0001, 0x0003).

    Note: These indicate the HPKE algorithm identifiers, specifying the KEM
        function as DHKEM(X25519, HKDF-SHA256), the KDF function as HKDF-SHA256
        and the AEAD function as ChaCha20Poly1305.
1. Let |hpkeContext| be the result of setting up an [[RFC9180|HPKE]]
    [[RFC9180#name-encryption-to-a-public-key|sender's context]] with |pkR|,
    |info|, |kem_id|, |kdf_id| and |aead_id|.
1. Let |aad| be \`\` (an empty [=byte sequence=]).
1. Let |encryptedPayload| be the result of
    [[RFC9180#name-encryption-and-decryption|encrypting]] |plaintextPayload|
    with |hpkeContext| and |aad|.

To <dfn>encode an integer for the payload</dfn> given an integer |intToEncode|
and an integer |bitLength|, return the representation of |intToEncode| as a
big-endian [=byte sequence=] of length |bitLength| / 8, left padding with zeroes
as necessary.

To <dfn>obtain a report's shared info</dfn> given an [=aggregatable report=]
|report|, perform the following steps. They return a [=string=].
1. Let |scheduledReportTime| be the [=duration from=] the [=UNIX epoch=] to
    |report|'s [=aggregatable report/original report time=].
1. Let |sharedInfo| be an [=ordered map=] of the following key/value pairs:
    : "`api`"
    :: |report|'s [=aggregatable report/api=]
    : "`report_id`"
    :: |report|'s [=aggregatable report/report ID=]
    : "`reporting_origin`"
    :: The [=serialization of an origin|serialization=] of |report|'s
        [=aggregatable report/reporting origin=]
    : "`scheduled_report_time`"
    :: The number of seconds in |scheduledReportTime|, rounded down to the
        nearest number of whole seconds and [=serialize an integer|serialized=]
    : "`version`"
    :: "`0.1`"
1. Return the result of [=serializing an infra value to a json string=] given
    |sharedInfo|.

Shared Storage API monkey patches {#shared-storage-api-monkey-patches}
======================================================================

Issue(43): This should be moved to the Shared Storage spec.

<xmp class="idl">
partial interface SharedStorageWorkletGlobalScope {
  readonly attribute PrivateAggregation privateAggregation;
};
</xmp>

The {{WindowSharedStorage}}'s {{WindowSharedStorage/run()}} method steps are
modified in two ways. First, add the following step in the nested scope just
after "Let |operation| be |operationMap|[|name|]." (renumbering later steps as
appropriate):
<div algorithm="shared-storage-run-monkey-patch-1">
2. Let <var ignore>batchingScope</var> be a new [=batching scope=].
1. Let <var ignore>debugScope</var> be a new [=debug scope=].

</div>
Second, at the end of the same nested scope, add the following step:
<div algorithm="shared-storage-run-monkey-patch-2">
6. When the above [=call=] returns, perform the following steps:
    1. [=Mark a debug scope complete=] given <var ignore>debugScope</var>.
    1. [=Process contributions for a batching scope=] given
        <var ignore>batchingScope</var>, <var ignore>outsideSettings</var>'
        [=environment settings object/origin=] and
        "<code>shared-storage</code>".

</div>

The {{WindowSharedStorage}}'s {{WindowSharedStorage/selectURL()}} method steps
are modified in two ways. First, add the following step in the nested scope just
after "Let |operation| be |operationMap|[|name|]." (renumbering later steps as
appropriate):
<div algorithm="shared-storage-selecturl-monkey-patch-1">
2. Let <var ignore>batchingScope</var> be a new [=batching scope=].
1. Let <var ignore>debugScope</var> be a new [=debug scope=].

</div>
Second, at the end of the same nested scope, add the following steps:
<div algorithm="shared-storage-selecturl-monkey-patch-2">
9. [=Mark a debug scope complete=] given <var ignore>debugScope</var>.
1. [=Process contributions for a batching scope=] given
    <var ignore>batchingScope</var>, <var ignore>outsideSettings</var>'
    [=environment settings object/origin=] and "<code>shared-storage</code>".

</div>

Issue: Once <a href="https://github.com/wicg/shared-storage/issues/88">
    shared-storage/88</a> is resolved, align the above monkey patches with how
    `keepAlive` is handled at operation completion.

The {{Worklet/addModule()}} steps are modified to add a new step just before
the final step ("Return <var ignore>promise</var>."), renumbering the last step
as appropriate:
<div algorithm="shared-storage-addmodule-monkey-patch">
7. If |this| is a {{SharedStorageWorklet}}, [=upon fulfillment=] of |promise| or
    [=upon rejection=] of |promise|, run the following steps:
    1. Let |globalScopes| be |this|'s [=Worklet/global scopes=].
    1. [=Assert=]: |globalScopes|' [=list/size=] equals 1.
    1. Set |globalScopes|[0]'s {{SharedStorageWorkletGlobalScope/
        privateAggregation}}'s [=PrivateAggregation/scoping details=] a new
            [=scoping details=] with the items:
        : [=scoping details/get batching scope steps=]
        :: An algorithm that returns the [=batching scope=] that is scheduled to
            be passed to [=process contributions for a batching scope=] when the
            call currently executing in |scope| returns.
        : [=scoping details/get debug scope steps=]
        :: An algorithm that returns the [=debug scope=] that is scheduled to be
            passed to [=mark a debug scope complete=] when the call currently
            executing in |scope| returns.

        Note: Multiple operation invocations can be in-progress at the same
        time, each with a different batching scope and debug scope. However,
        only one can be currently executing.

</div>

Issue: Once <a href="https://github.com/wicg/shared-storage/issues/89">
    shared-storage/89</a> is resolved, align the above monkey patch with how
    access to `sharedStorage` is prevented in
    {{SharedStorageWorkletGlobalScope}}s until {{Worklet/addModule()}}'s initial
    execution is complete.

Note: This extends Shared Storage's existing {{Worklet/addModule()}}
    <a href="https://wicg.github.io/shared-storage/#worklet-monkey-patch">
    monkey patch</a>.

Protected Audience API monkey patches {#protected-audience-api-monkey-patches}
==============================================================================

Issue(43): This should be moved to the Protected Audience spec, along with any
    other Protected Audience-specific details.

<xmp class="idl">
partial interface InterestGroupScriptRunnerGlobalScope {
  readonly attribute PrivateAggregation privateAggregation;
};

dictionary PASignalValue {
  required DOMString baseValue;
  double scale;
  (bigint or long) offset;
};

dictionary PAExtendedHistogramContribution {
  required (PASignalValue or bigint) bucket;
  required (PASignalValue or long) value;
};

[Exposed=InterestGroupScriptRunnerGlobalScope, SecureContext]
partial interface PrivateAggregation {
  undefined contributeToHistogramOnEvent(
      DOMString event, PAExtendedHistogramContribution contribution);
};
</xmp>

Issue: Do we want to align naming with implementation?

The {{Navigator/runAdAuction()}} method steps are modified to add the following
steps after step 6 ("Let <var ignore>p</var> be [=a new promise=]"), renumbering
later steps as appropriate:
<div algorithm="protected-audience-runadauction-monkey-patch">
7. Let |batchingScopeMap| be a new [=map/is empty|empty=] [=map=].
1. [=list/iterate|For each=] |bidderOrigin| of |auctionConfig|'s
    <a spec="turtledove" for="auction config">interest group buyers</a>:
    1. [=map/Set=] |batchingScopeMap|[|bidderOrigin|] to a new [=batching
        scope=].
1. Let |sellerOrigin| be |auctionConfig|'s
    <a spec="turtledove" for="auction config">seller</a>.
1. [=map/Set=] |batchingScopeMap|[|sellerOrigin|] to a new [=batching scope=].
1. [=Upon fulfillment=] of |p| or [=upon rejection=] of |p|, run the following
    steps:
    1. TODO: Take onEvent contributions from its cache, run [=fill in the
        contribution=] and then [=append an entry to the contribution cache=].
    1. [=list/iterate|For each=] |origin| → |batchingScope| of
        |batchingScopeMap|:
        1. [=Process contributions for a batching scope=] given |batchingScope|,
            |origin| and "<code>protected-audience</code>".

</div>

Issue: How to handle fenced frame-triggered contributions and other
    event-triggered contributions.

Issue: Need to handle `auctionReportBuyers` and `auctionReportBuyerKeys` here or
    in the Protected Audience API spec.

The <a spec="turtledove">evaluate a script</a> steps are modified in two ways.
First, the following steps are added after step 11 ("If
<var ignore>evaluationStatus</var> is an [=ECMAScript/abrupt completion=]..."),
renumbering later steps as appropriate:
<div algorithm="protected-audience-evaluate-a-script-monkey-patch-1">
12. Let |debugScope| be a new [=debug scope=].
1. Set <var ignore>global</var>'s {{InterestGroupScriptRunnerGlobalScope/
    privateAggregation}}'s [=PrivateAggregation/scoping details=] to a new
        [=scoping details=] with the items:
    : [=scoping details/get batching scope steps=]
    :: An algorithm that returns the [=batching scope=] associated with the
        auction and the <var ignore>scope</var>'s [=relevant settings object=]'s
        [=environment settings object/origin=].
    : [=scoping details/get debug scope steps=]
    :: An algorithm that returns |debugScope|.

</div>
Second, in the nested scope of the last step, we insert a new step just after
the step labelled "Clean up after script", renumbering the later step as
appropriate:
<div algorithm="protected-audience-evaluate-a-script-monkey-patch-2">
2. [=Mark a debug scope complete=] given <var ignore>debugScope</var>.

</div>

Issue: Once <a href="https://github.com/wicg/turtledove/issues/615">
    protected-audience/615</a> is resolved, align the above monkey patch with
    how access to other functions is prevented in
    {{InterestGroupScriptRunnerGlobalScope}}s until the script's initial
    execution is complete.

<div algorithm>
The <dfn method for="PrivateAggregation">contributeToHistogramOnEvent(DOMString
event, PAExtendedHistogramContribution contribution)</dfn> method steps are:
</div>

Issue: Fill in validation. Will need to check offsets are valid and in the right
    range for the type etc. Also that base values are one of a set.

Issue: Need to document limits on offset, etc.

Issue: Fill in the rest. (Need to put the contribution in some sort of queue and
    process the queue at some point. Need to decide where the queue should live
    given it has to outlive the auction.)

Issue(44): Consider accepting an array of contributions.

Protected Audience API-specific structures {#protected-audience-api-specific-structures}
----------------------------------------------------------------------------------------

<h4 dfn-type=dfn>Signal base value</h3>
A signal base value is one of the following:
<dl dfn-for="signal base value">
: "<dfn export><code>winning-bid</code></dfn>"
:: The bid value of the winning bid.
: "<dfn export><code>highest-scoring-other-bid</code></dfn>"
:: The bid value of the highest scoring bid that did not win.
: "<dfn export><code>script-run-time</code></dfn>"
:: The running time of the script in milliseconds.
: "<dfn export><code>signals-fetch-time</code></dfn>"
:: The time it took for the signals fetch to complete in milliseconds.
: "<dfn export><code>bid-reject-reason</code></dfn>"
:: The reason a bid was rejected.

</dl>

Issue: Remove exports when these definitions are used.

Issue: Make sure these definitions match "determine the numeric value" algorithm

Issue: New enum needed for bid reject reasons.

Protected Audience API-specific algorithms {#protected-audience-api-specific-algorithms}
----------------------------------------------------------------------------------------

To <dfn>fill in the contribution</dfn> given a |contribution|, perform the
following steps. They return a {{PAHistogramContribution}}.
1. If |contribution| is a {{PAHistogramContribution}}, return |contribution|.
1. Otherwise, [=assert=]: |contribution| is a
    {{PAExtendedHistogramContribution}}.
1. Let |bucket| be |contribution|["{{PAExtendedHistogramContribution/bucket}}"].
1. If |bucket| is a {{PASignalValue}}, set |bucket| to the result of [=filling
    in the signal value=] given |bucket| and 65535.
1. Let |value| be |contribution|["{{PAExtendedHistogramContribution/value}}"].
1. If |value| is a {{PASignalValue}}, set |value| to the result of [=filling in
    the signal value=] given |value| and 2<sup>128</sup>−1.
1. Return a new {{PAHistogramContribution}} with the items:
    : {{PAHistogramContribution/bucket}}
    :: |bucket|
    : {{PAHistogramContribution/value}}
    :: |value|

To <dfn>fill in the signal value</dfn> given a {{PASignalValue}} |value| and an
integer |maxAllowed|, perform the following steps. They return an integer.
1. [=Assert=]: |value|["{{PASignalValue/baseValue}}"] is a valid [=signal base
    value=].
1. Let |returnValue| be the result of [=determining the numeric value=] of
    |value|["{{PASignalValue/baseValue}}"].
1. If |value|["{{PASignalValue/scale}}"] [=map/exists=], set |returnValue| to
    the result of multiplying |value|["{{PASignalValue/scale}}"] with
    |returnValue|.
1. Set |returnValue| to the integer result of rounding |returnValue| to the
    nearest integer. If two integers are equally close, the result should be the
    integer closer to negative infinity.
1. If |value|["{{PASignalValue/offset}}"] [=map/exists=], set |returnValue| to
    the result of adding |returnValue| to |value|["{{PASignalValue/offset}}"].
1. Clamp |returnValue| to the range [0, |maxAllowed|] and return the result.

Issue: Maybe add refs to the rounding logic.

To <dfn>determine the numeric value</dfn> of a [=signal base value=]
<var ignore=''>signalBaseValue</var>, perform the following steps. They return a
{{double}}.

Issue: Fill in.

Privacy considerations {#privacy-considerations}
================================================

<em>This section is non-normative.</em>

Cross-site information disclosure {#cross-site-information-disclosure}
----------------------------------------------------------------------

This API lets isolated contexts with access to cross-site data (i.e. <a
href="https://wicg.github.io/shared-storage/">Shared Storage</a> worklets/<a
href="https://wicg.github.io/turtledove/">Protected Audience</a> script runners)
send aggregatable reports over the network.

Aggregatable reports contain encrypted high entropy cross-site information, in
the form of key-value pairs (i.e. contributions to a histogram). The information
embedded in the contributions is arbitrary but can include things like browsing
history and other cross-site activity. The API aims to protect this information
from being passed from one site to another.

### Restricted contribution processing ### {#restricted-contribution-processing}

The histogram contributions are not exposed directly. Instead, they are
encrypted so that they can only be processed by a trusted aggregation service.
This trusted aggregation service sums the values across the reports for each key
and adds noise to each of these values to produce ‘summary reports’.

The output of that processing will be an aggregated, noised histogram. The
service ensures that any report can not be processed multiple times. Further,
information exposure is limited by contribution budgets on the user agent. In
principle, this framework can support specifying a noise parameter which
satisfies differential privacy.

### Unencrypted metadata ### {#unencrypted-metadata}

These reports also expose a limited amount of metadata, which is not based on
cross-site data. The recipient of the report may also be able to observe
side-channel information such as the time when the report was sent, or IP
address of the sender.

### Protecting against leaks via the number of reports ### {#protecting-against-leaks-via-the-number-of-reports}

However, the number of reports with the given metadata could expose some
cross-site information. To protect against this, the API delays sending reports
by a randomized amount of time to make it difficult to determine whether a
report was sent or not from any particular event. In the case that a context ID
is supplied, the API makes the number of reports sent deterministic (sending
reports containing no contributions in the payloads if necessary). Additional
mitigations may also be possible in the future, e.g. adding noise to the report
count.

### Protecting against leaks via payload size ### {#protecting-against-leaks-via-payload-size}

The length of the payload could additionally expose some cross-site information,
namely how many contributions are included. To protect against this, the payload
will be padded in the future.

Issue(56): Pad the payload to avoid this risk.

### Temporary debugging mechanism ### {#temporary-debugging-mechanism}

The <code>{{PrivateAggregation/enableDebugMode()}}</code> method allows for many
of the protections of this API to be bypassed to ease testing and integration.
Specifically, the contents of the payload, i.e. the histogram contributions, are
revealed in the clear when the debug mode is enabled. Optionally, a debug key
can also be set to associate the report with the calling context. In the future,
this mechanism will only be available for callers that are eligible to set
third-party cookies. In that case, the API caller already has the ability to
communicate information cross-site.

Issue(57): Tie <code>{{PrivateAggregation/enableDebugMode()}}</code> to
    third-party cookie eligibility.

### Privacy parameters ### {#privacy-parameters}

The amount of information exposed by this API is a product of the privacy
parameters used (e.g. contribution limits and the noise distribution used in the
aggregation service). While we aim to minimize the amount of information
exposed, we also aim to support a wide range of use cases. The privacy
parameters are left [=implementation-defined=] to allow different and evolving
choices in the tradeoffs between information exposure and utility.

Clearing site data {#clearing-site-data}
----------------------------------------

The [=aggregatable report cache=] as well as any contribution history data
stored for the [=consume budget if permitted=] algorithm contain data about a
user’s web activity. As such, user controls to delete this data are required,
see [clearing storage](#clearing-storage).

On the other hand, the [=contribution cache=] and [=debug scope map=] only
contain short-lived data tied to particular [=batching scopes=] and [=debug
scopes=], so controls are not required.

Reporting delay concerns {#reporting-delay-concerns}
----------------------------------------------------

Delaying sending reports after API invocation can enable side-channel leakage in
some situations.

### Cross-network reporting origin leakage ### {#cross-network-reporting-origin-leakage}

A report may be stored while the browser is connected to one network but sent
while the browser is connected to a different network, potentially enabling
cross-network leakage of the reporting origin.

Example: A user runs the browser with a particular browsing profile on their
home network. An aggregatable report with a particular reporting origin is
stored with a report time in the future. After the report time is reached, the
user runs the browser with the same browsing profile on their employer’s
network, at which point the browser sends the report to the reporting origin.
Although the report itself may be sent over HTTPS, the reporting origin may be
visible to the network administrator via DNS or the TLS client hello (which can
be mitigated with ECH). Some reporting origins may be known to operate only or
primarily on sensitive sites, so this could leak information about the user’s
browsing activity to the user’s employer without their knowledge or consent.

Possible mitigations include:

1. Only sending reports with a given reporting origin when the browser has
    already made a request to that origin on the same network: This prevents the
    network administrator from gaining additional information from the Private
    Aggregation API. However, it increases report loss and report delays, which
    reduces the utility of the API for the reporting origin. It might also
    increase the effectiveness of timing attacks, as the origin may be able to
    better link the report with the user’s request that allowed the report to be
    released.
1. Send reports immediately: This reduces the likelihood of a report being
    stored and sent on different networks. However, it increases the likelihood
    that the reporting origin can correlate the original API invocation to the
    report being sent, which weakens the privacy controls of the API, see
    [Protecting against leaks via the number of
    reports](#protecting-against-leaks-via-the-number-of-reports).
1. Use a trusted proxy server to send reports: This effectively moves the
    reporting origin into the report body, so only the proxy server would be
    visible to the network administrator.
1. Require [DNS over HTTPS](https://datatracker.ietf.org/doc/html/rfc8484): This
    effectively hides the reporting origin from the network administrator, but
    is likely impractical to enforce and is itself perhaps circumventable by the
    network administrator, e.g. by monitoring IP addresses instead.

### User-presence tracking ### {#user-presence-tracking}

The browser only tries to send reports while it is running and while it has
internet connectivity (even without an explicit check for connectivity,
naturally the report will fail to be sent if there is none), so receiving or not
receiving a ([=serializing an aggregatable report|serialized=]) [=aggregatable
report=] at the [=aggregatable report/original report time=] leaks information
about the user’s presence. Additionally, because the report request inherently
includes an IP address, this could reveal the user’s IP-derived location to the
reporting origin, including at-home vs. at-work or approximate real-world
geolocation, or reveal patterns in the user’s browsing activity.

Possible mitigations include:

1. Send reports immediately: This effectively eliminates the presence tracking,
    as the original request made to the reporting origin is in close temporal
    proximity to the report request. However, it increases the likelihood that
    the reporting origin can correlate the original API invocation to the report
    being sent, which weakens the privacy controls of the API, see [Protecting
    against leaks via the number of
    reports](#protecting-against-leaks-via-the-number-of-reports).
1. Send reports immediately to a trusted proxy server, which would itself apply
    additional delay: This would effectively hide both the user’s IP address and
    their online-offline presence from the reporting origin.

Security considerations {#security-considerations}
================================================

<em>This section is non-normative.</em>

Same-origin policy {#same-origin-policy}
----------------------------------------

Writes to the [=aggregatable report cache=] and [=contribution cache=] are
separated by the reporting [=origin=] and the data included in any report with a
given reporting [=origin=] are generated with only data from that [=origin=].

One notable exception is the [=consume budget if permitted=] algorithm which is
[=implementation-defined=] and can consider contribution history from other
[=origins=]. For example, the algorithm could consider all history from a
particular [=site=]. This would be an explicit relaxation of the same-origin
policy as multiple origins would be able to influence the API's behavior. One
particular risk of these kinds of shared limits is the introduction of denial of
service attacks, where a group of origins could collude to intentionally consume
all available budget, causing subsequent origins to be unable to access the API.
This trades off security for privacy, in that the limits are there to reduce the
efficacy of many origins colluding together to violate privacy. However, this
security risk is lessened if the set of origins limited are all [=same site=].
[=User agents=] should consider these tradeoffs when choosing the [=consume
budget if permitted=] algorithm.

Protecting the histogram contributions {#protecting-the-histogram-contributions}
--------------------------------------------------------------------------------

As discussed [above](#restricted-contribution-processing), the processing of
histogram contributions is limited to protect privacy. This limitation relies on
only the trusted aggregation service being able to access the unencrypted
histogram contributions.

To ensure this, this API uses [[RFC9180|HPKE]], a modern encryption
specification. Additionally, each [=user agent=] is encouraged to require
regular key rotation by the aggregation service. This limits the amount of data
encrypted with the same key and thus the amount of vulnerable data in the case
of a key being compromised.

While not specified here, each [=user agent=] is strongly encouraged to consider
the security of any aggregation service design before allowing its public keys
to be returned by [=obtain the public key for encryption=].
